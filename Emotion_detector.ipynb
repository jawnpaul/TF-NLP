{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to ensure TensorFlow 2.x is used\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/john/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7666\n",
      "7666\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "labelsList = []\n",
    "\n",
    "#raw_data = pd.read_csv('/content/drive/My Drive/new_transaction.csv')\n",
    "\n",
    "with open(\"isear.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labelsList.append(row[0])\n",
    "        article = row[0]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "            article = article.replace(' ', ' ')\n",
    "        articles.append(article)\n",
    "print(len(labelsList))\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 199: expected 43 fields, saw 44\\nSkipping line 222: expected 43 fields, saw 44\\nSkipping line 248: expected 43 fields, saw 44\\nSkipping line 350: expected 43 fields, saw 44\\nSkipping line 395: expected 43 fields, saw 44\\nSkipping line 423: expected 43 fields, saw 44\\nSkipping line 480: expected 43 fields, saw 44\\nSkipping line 552: expected 43 fields, saw 44\\nSkipping line 601: expected 43 fields, saw 45\\nSkipping line 604: expected 43 fields, saw 45\\nSkipping line 612: expected 43 fields, saw 45\\nSkipping line 722: expected 43 fields, saw 44\\nSkipping line 765: expected 43 fields, saw 44\\nSkipping line 800: expected 43 fields, saw 46\\nSkipping line 818: expected 43 fields, saw 45\\nSkipping line 891: expected 43 fields, saw 44\\nSkipping line 928: expected 43 fields, saw 44\\nSkipping line 963: expected 43 fields, saw 44\\nSkipping line 965: expected 43 fields, saw 45\\nSkipping line 974: expected 43 fields, saw 44\\nSkipping line 985: expected 43 fields, saw 44\\nSkipping line 1064: expected 43 fields, saw 44\\nSkipping line 1068: expected 43 fields, saw 46\\nSkipping line 1090: expected 43 fields, saw 45\\nSkipping line 1091: expected 43 fields, saw 44\\nSkipping line 1174: expected 43 fields, saw 44\\nSkipping line 1175: expected 43 fields, saw 45\\nSkipping line 1179: expected 43 fields, saw 44\\nSkipping line 1193: expected 43 fields, saw 44\\nSkipping line 1233: expected 43 fields, saw 44\\nSkipping line 1239: expected 43 fields, saw 44\\nSkipping line 1301: expected 43 fields, saw 44\\nSkipping line 1312: expected 43 fields, saw 45\\nSkipping line 1320: expected 43 fields, saw 44\\nSkipping line 1349: expected 43 fields, saw 44\\nSkipping line 1381: expected 43 fields, saw 45\\nSkipping line 1439: expected 43 fields, saw 46\\nSkipping line 1460: expected 43 fields, saw 45\\nSkipping line 1465: expected 43 fields, saw 46\\nSkipping line 1466: expected 43 fields, saw 44\\nSkipping line 1476: expected 43 fields, saw 45\\nSkipping line 1489: expected 43 fields, saw 46\\nSkipping line 1506: expected 43 fields, saw 44\\nSkipping line 1523: expected 43 fields, saw 44\\nSkipping line 1539: expected 43 fields, saw 44\\nSkipping line 1550: expected 43 fields, saw 44\\nSkipping line 1576: expected 43 fields, saw 44\\nSkipping line 1703: expected 43 fields, saw 44\\nSkipping line 1712: expected 43 fields, saw 45\\nSkipping line 1781: expected 43 fields, saw 44\\nSkipping line 1946: expected 43 fields, saw 46\\nSkipping line 1950: expected 43 fields, saw 44\\nSkipping line 2038: expected 43 fields, saw 44\\nSkipping line 2047: expected 43 fields, saw 44\\nSkipping line 2048: expected 43 fields, saw 44\\nSkipping line 2060: expected 43 fields, saw 44\\nSkipping line 2083: expected 43 fields, saw 44\\nSkipping line 2230: expected 43 fields, saw 44\\nSkipping line 2241: expected 43 fields, saw 46\\nSkipping line 2245: expected 43 fields, saw 45\\nSkipping line 2250: expected 43 fields, saw 45\\nSkipping line 2255: expected 43 fields, saw 46\\nSkipping line 2272: expected 43 fields, saw 45\\nSkipping line 2356: expected 43 fields, saw 44\\nSkipping line 2358: expected 43 fields, saw 44\\nSkipping line 2412: expected 43 fields, saw 45\\nSkipping line 2432: expected 43 fields, saw 49\\nSkipping line 2538: expected 43 fields, saw 46\\nSkipping line 2548: expected 43 fields, saw 44\\nSkipping line 2558: expected 43 fields, saw 45\\nSkipping line 2569: expected 43 fields, saw 45\\nSkipping line 2619: expected 43 fields, saw 44\\nSkipping line 2659: expected 43 fields, saw 44\\nSkipping line 2688: expected 43 fields, saw 44\\nSkipping line 2722: expected 43 fields, saw 45\\nSkipping line 2746: expected 43 fields, saw 44\\nSkipping line 2780: expected 43 fields, saw 44\\nSkipping line 2788: expected 43 fields, saw 44\\nSkipping line 2804: expected 43 fields, saw 44\\nSkipping line 2815: expected 43 fields, saw 44\\nSkipping line 2817: expected 43 fields, saw 44\\nSkipping line 2848: expected 43 fields, saw 44\\nSkipping line 2903: expected 43 fields, saw 44\\nSkipping line 2983: expected 43 fields, saw 44\\nSkipping line 2999: expected 43 fields, saw 45\\nSkipping line 3013: expected 43 fields, saw 44\\nSkipping line 3060: expected 43 fields, saw 44\\nSkipping line 3069: expected 43 fields, saw 45\\nSkipping line 3180: expected 43 fields, saw 45\\nSkipping line 3190: expected 43 fields, saw 45\\nSkipping line 3193: expected 43 fields, saw 46\\nSkipping line 3194: expected 43 fields, saw 45\\nSkipping line 3195: expected 43 fields, saw 45\\nSkipping line 3199: expected 43 fields, saw 44\\nSkipping line 3210: expected 43 fields, saw 44\\nSkipping line 3269: expected 43 fields, saw 44\\nSkipping line 3279: expected 43 fields, saw 44\\nSkipping line 3319: expected 43 fields, saw 44\\nSkipping line 3327: expected 43 fields, saw 44\\nSkipping line 3380: expected 43 fields, saw 44\\nSkipping line 3448: expected 43 fields, saw 47\\nSkipping line 3471: expected 43 fields, saw 44\\nSkipping line 3490: expected 43 fields, saw 44\\nSkipping line 3493: expected 43 fields, saw 44\\nSkipping line 3511: expected 43 fields, saw 44\\nSkipping line 3627: expected 43 fields, saw 45\\nSkipping line 3635: expected 43 fields, saw 44\\nSkipping line 3670: expected 43 fields, saw 44\\nSkipping line 3684: expected 43 fields, saw 44\\nSkipping line 3699: expected 43 fields, saw 44\\nSkipping line 3707: expected 43 fields, saw 44\\nSkipping line 3748: expected 43 fields, saw 44\\nSkipping line 3772: expected 43 fields, saw 44\\nSkipping line 3787: expected 43 fields, saw 45\\nSkipping line 3872: expected 43 fields, saw 45\\nSkipping line 3878: expected 43 fields, saw 44\\nSkipping line 4011: expected 43 fields, saw 44\\nSkipping line 4029: expected 43 fields, saw 44\\nSkipping line 4147: expected 43 fields, saw 44\\nSkipping line 4312: expected 43 fields, saw 44\\nSkipping line 4407: expected 43 fields, saw 45\\nSkipping line 4604: expected 43 fields, saw 44\\nSkipping line 4627: expected 43 fields, saw 45\\nSkipping line 4693: expected 43 fields, saw 44\\nSkipping line 4694: expected 43 fields, saw 45\\nSkipping line 4695: expected 43 fields, saw 44\\nSkipping line 4697: expected 43 fields, saw 44\\nSkipping line 4742: expected 43 fields, saw 44\\nSkipping line 4744: expected 43 fields, saw 46\\nSkipping line 4756: expected 43 fields, saw 44\\nSkipping line 4763: expected 43 fields, saw 44\\nSkipping line 4868: expected 43 fields, saw 44\\nSkipping line 4909: expected 43 fields, saw 45\\nSkipping line 4963: expected 43 fields, saw 44\\nSkipping line 5026: expected 43 fields, saw 44\\nSkipping line 5124: expected 43 fields, saw 44\\nSkipping line 5168: expected 43 fields, saw 45\\nSkipping line 5202: expected 43 fields, saw 45\\nSkipping line 5255: expected 43 fields, saw 46\\nSkipping line 5290: expected 43 fields, saw 45\\nSkipping line 5295: expected 43 fields, saw 46\\nSkipping line 5500: expected 43 fields, saw 45\\nSkipping line 5511: expected 43 fields, saw 44\\nSkipping line 5558: expected 43 fields, saw 44\\nSkipping line 5560: expected 43 fields, saw 48\\nSkipping line 5709: expected 43 fields, saw 44\\nSkipping line 5835: expected 43 fields, saw 45\\nSkipping line 6070: expected 43 fields, saw 47\\nSkipping line 6101: expected 43 fields, saw 44\\nSkipping line 6105: expected 43 fields, saw 45\\nSkipping line 6107: expected 43 fields, saw 45\\nSkipping line 6336: expected 43 fields, saw 45\\nSkipping line 7115: expected 43 fields, saw 44\\nSkipping line 7125: expected 43 fields, saw 44\\nSkipping line 7339: expected 43 fields, saw 48\\nSkipping line 7483: expected 43 fields, saw 46\\nSkipping line 7487: expected 43 fields, saw 44\\nSkipping line 7498: expected 43 fields, saw 46\\nSkipping line 7584: expected 43 fields, saw 52\\nSkipping line 7598: expected 43 fields, saw 44\\nSkipping line 7617: expected 43 fields, saw 46\\nSkipping line 7654: expected 43 fields, saw 48\\nSkipping line 7656: expected 43 fields, saw 46\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"isear.csv\", delimiter=\"|\" , error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COUN</th>\n",
       "      <th>SUBJ</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>RELI</th>\n",
       "      <th>PRAC</th>\n",
       "      <th>FOCC</th>\n",
       "      <th>MOCC</th>\n",
       "      <th>...</th>\n",
       "      <th>RELA</th>\n",
       "      <th>VERBAL</th>\n",
       "      <th>NEUTRO</th>\n",
       "      <th>Field1</th>\n",
       "      <th>Field3</th>\n",
       "      <th>Field2</th>\n",
       "      <th>MYKEY</th>\n",
       "      <th>SIT</th>\n",
       "      <th>STATE</th>\n",
       "      <th>Unnamed: 42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>110011</td>\n",
       "      <td>During the period of falling in love, each tim...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>110012</td>\n",
       "      <td>When I was involved in a traffic accident.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110013</td>\n",
       "      <td>When I was driving home after  several days of...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>110014</td>\n",
       "      <td>When I lost the person who meant the most to me.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>disgust</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>110015</td>\n",
       "      <td>The time I knocked a deer down - the sight of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  CITY  COUN  SUBJ  SEX  AGE  RELI  PRAC  FOCC  MOCC  ...  RELA  \\\n",
       "0  11001     1     1     1    1   33     1     2     6     1  ...     3   \n",
       "1  11001     1     1     1    1   33     1     2     6     1  ...     2   \n",
       "2  11001     1     1     1    1   33     1     2     6     1  ...     1   \n",
       "3  11001     1     1     1    1   33     1     2     6     1  ...     1   \n",
       "4  11001     1     1     1    1   33     1     2     6     1  ...     2   \n",
       "\n",
       "   VERBAL  NEUTRO   Field1  Field3  Field2   MYKEY  \\\n",
       "0       2       0      joy       4       3  110011   \n",
       "1       0       0     fear       3       2  110012   \n",
       "2       0       0    anger       1       3  110013   \n",
       "3       0       2  sadness       4       4  110014   \n",
       "4       0       0  disgust       4       4  110015   \n",
       "\n",
       "                                                 SIT  STATE  Unnamed: 42  \n",
       "0  During the period of falling in love, each tim...      1          NaN  \n",
       "1         When I was involved in a traffic accident.      1          NaN  \n",
       "2  When I was driving home after  several days of...      1          NaN  \n",
       "3  When I lost the person who meant the most to me.       1          NaN  \n",
       "4  The time I knocked a deer down - the sight of ...      1          NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID\n",
       "0  11001\n",
       "1  11001\n",
       "2  11001\n",
       "3  11001\n",
       "4  11001"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.drop(['ID', 'CITY', 'COUN', 'SUBJ','SEX', 'PRAC', 'MOCC','AGE','RELI','FOCC','FIEL'], axis = 1) \n",
    "\n",
    "# Remove all columns between column index 1 to 19\n",
    "data.drop(data.iloc[:, 1:19], inplace = True, axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7503\n",
      "['During the period of falling in love, each time that we met and á especially when we had not met for a long time.', 'When I was involved in a traffic accident.', 'When I was driving home after  several days of hard work, there á was a motorist ahead of me who was driving at 50 km/hour and á refused, despite his low speeed to let me overtake.']\n"
     ]
    }
   ],
   "source": [
    "df2 = data.loc[:,'Field1':'SIT']\n",
    "df2 = df2.drop(['Field3', 'Field2', 'MYKEY'], axis = 1)\n",
    "#df2.head()\n",
    "\n",
    "emotion_list = df2['Field1'].tolist()\n",
    "sentence_list = df2['SIT'].tolist()\n",
    "#Field1 to emotionlist, SIT to sentenceList, \n",
    "\n",
    "print(len(sentence_list))\n",
    "#articless = []\n",
    "\n",
    "# for each sentence in sentence list, I wnat to check if it contains a stopword, if yes replace the word with ' '\n",
    "#for sentence in sentence_list:\n",
    " #   for word in STOPWORDS:\n",
    "  #      token = ' ' + word + ' '\n",
    "   #     sentence = sentence.replace(token, ' ')\n",
    "    #    sentence = sentence.replace(' ', ' ')\n",
    "    #sentence_list.append(sentence) \n",
    "\n",
    "#print(len(sentence_list))\n",
    "\n",
    "print(sentence_list[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['During the period of falling in love, each time that we met and á especially when we had not met for a long time.', 'When I was involved in a traffic accident.', 'When I was driving home after  several days of hard work, there á was a motorist ahead of me who was driving at 50 km/hour and á refused, despite his low speeed to let me overtake.']\n"
     ]
    }
   ],
   "source": [
    "for word in STOPWORDS:\n",
    "    token = ' ' + word + ' '\n",
    "    res = list(map(lambda token: str.replace(token, ' ', ' '), sentence_list))\n",
    "print(res[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7503"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceFiltered = []\n",
    "for w in sentence_list:\n",
    "    if w not in STOPWORDS:\n",
    "        sentenceFiltered.append(w)\n",
    "len(sentenceFiltered)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ress = [map(lambda word: str.replace(word, ' ', ' '), sentence_list) for word in STOPWORDS]\n",
    "ad = [list(map(lambda word: str.replace(word, ' ', ' '), sentence_list)) for word in STOPWORDS]\n",
    "len(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you'll\n",
      "yourself\n",
      "here\n",
      "who\n",
      "aren't\n",
      "them\n",
      "couldn't\n",
      "being\n",
      "until\n",
      "those\n",
      "isn\n",
      "do\n",
      "needn\n",
      "than\n",
      "to\n",
      "out\n",
      "herself\n",
      "a\n",
      "shan't\n",
      "be\n",
      "any\n",
      "all\n",
      "you're\n",
      "how\n",
      "s\n",
      "was\n",
      "yours\n",
      "this\n",
      "haven't\n",
      "most\n",
      "these\n",
      "myself\n",
      "hadn't\n",
      "off\n",
      "weren\n",
      "have\n",
      "can\n",
      "wouldn\n",
      "other\n",
      "ours\n",
      "she's\n",
      "up\n",
      "our\n",
      "been\n",
      "hasn't\n",
      "mightn't\n",
      "isn't\n",
      "does\n",
      "y\n",
      "shouldn\n",
      "between\n",
      "d\n",
      "as\n",
      "or\n",
      "about\n",
      "now\n",
      "doesn't\n",
      "into\n",
      "nor\n",
      "there\n",
      "m\n",
      "aren\n",
      "he\n",
      "further\n",
      "didn't\n",
      "am\n",
      "are\n",
      "won't\n",
      "weren't\n",
      "didn\n",
      "won\n",
      "haven\n",
      "very\n",
      "if\n",
      "having\n",
      "me\n",
      "more\n",
      "ll\n",
      "during\n",
      "after\n",
      "from\n",
      "too\n",
      "because\n",
      "wasn't\n",
      "you'd\n",
      "whom\n",
      "down\n",
      "hadn\n",
      "over\n",
      "its\n",
      "o\n",
      "should've\n",
      "shan\n",
      "then\n",
      "where\n",
      "by\n",
      "ma\n",
      "why\n",
      "same\n",
      "that\n",
      "couldn\n",
      "the\n",
      "shouldn't\n",
      "on\n",
      "doesn\n",
      "is\n",
      "your\n",
      "under\n",
      "you've\n",
      "few\n",
      "of\n",
      "above\n",
      "below\n",
      "will\n",
      "were\n",
      "hasn\n",
      "she\n",
      "itself\n",
      "so\n",
      "re\n",
      "such\n",
      "both\n",
      "wasn\n",
      "themselves\n",
      "it's\n",
      "once\n",
      "has\n",
      "when\n",
      "before\n",
      "his\n",
      "which\n",
      "only\n",
      "an\n",
      "with\n",
      "mustn\n",
      "while\n",
      "yourselves\n",
      "what\n",
      "himself\n",
      "needn't\n",
      "i\n",
      "it\n",
      "doing\n",
      "mightn\n",
      "some\n",
      "and\n",
      "ain\n",
      "ve\n",
      "ourselves\n",
      "not\n",
      "for\n",
      "t\n",
      "just\n",
      "did\n",
      "we\n",
      "no\n",
      "don\n",
      "theirs\n",
      "own\n",
      "they\n",
      "at\n",
      "you\n",
      "him\n",
      "their\n",
      "against\n",
      "in\n",
      "mustn't\n",
      "my\n",
      "should\n",
      "that'll\n",
      "through\n",
      "wouldn't\n",
      "hers\n",
      "but\n",
      "again\n",
      "had\n",
      "each\n",
      "don't\n",
      "her\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
